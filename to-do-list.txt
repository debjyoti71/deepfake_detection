| **Step**             | **Sub-Step**| **What to Do**                                         | **Which File/Folder**                    | **Why (Purpose)**                                         | **Done?** | 
| ---------------------| ------------| -------------------------------------------------------| -----------------------------------------| ----------------------------------------------------------| --------- | 
| **1. Setup**         | 1.1         | Create virtual environment & install requirements      | `requirements.txt`                       | To set up dependencies (TensorFlow, PyTorch, OpenCV, etc.)| ✅        | 
| **2. Preprocessing** | 2.1         | Extract frames from all videos                         | `src/preprocessing/extract_frames.py`    | Break videos into images for visual model                 | ✅        | 
|                      | 2.2         | Detect & crop faces from frames                        | `src/preprocessing/face_detection.py`    | Only keep face regions to reduce noise                    | 🟡        | 
|                      | 2.3         | Extract audio from videos                              | `src/preprocessing/extract_audio.py`     | Separate audio track for audio model                      | 🟡        | 
|                      | 2.4         | Convert audio → features (mel-spectrograms, embeddings)| `src/preprocessing/feature_extraction.py`| Prepare input for audio model                             | 🟡        | 
|                      | 2.5         | Align lips & audio (sync dataset)                      | `src/preprocessing/sync_preprocess.py`   | Create input for sync model                               | 🟡        | 
| **3. Training**      | 3.1         | Train visual deepfake detection model                  | `src/training/train_visual.py`           | Detect visual manipulations in frames                     | 🟡        | 
|                      | 3.2         | Train audio deepfake detection model                   | `src/training/train_audio.py`            | Detect fake voice/speech                                  | 🟡        | 
|                      | 3.3         | Train audio-visual sync model                          | `src/training/train_sync.py`             | Check mismatch between lips & speech                      | 🟡        | 
|                      | 3.4         | Train fusion model (combine outputs)                   | `src/training/train_fusion.py`           | Final classifier for 4 categories (A, B, C, D)            | 🟡        | 
| **4. Evaluation**    | 4.1         | Evaluate all models on test set                        | `src/evaluation/evaluate.py`             | Get accuracy, AUC, F1, EER                                | 🟡        | 
|                      | 4.2         | Generate explainability results (Grad-CAM, heatmaps)   | `src/evaluation/explainability.py`       | Visualize why model predicts fake/real                    | 🟡        | 
| **5. API Deployment**| 5.1         | Build inference endpoint `/predict`                    | `src/api/main.py`                        | Upload video → Get prediction (Real/Fake + Category)      | 🟡        | 
|                      | 5.2         | Test API with sample videos                            | `tests/test_api.py`                      | Check real-time inference                                 | ❌        | 
|                      | 5.3         | Deploy to cloud (Render/AWS/Vercel)                    | Deployment config                        | Make project usable outside your system                   | ❌        | 
