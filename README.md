# Deepfake Detection System

A comprehensive multi-modal deepfake detection system that analyzes both visual and audio components of videos to identify manipulated content. The system uses four specialized models to classify videos into four categories based on authenticity.

## üéØ Project Overview

This project detects deepfakes by analyzing:
- **Visual content**: Face manipulation detection
- **Audio content**: Voice synthesis detection  
- **Audio-Visual synchronization**: Lip-sync mismatch detection
- **Multi-modal fusion**: Combined decision making

### Classification Categories
- **A**: RealVideo-RealAudio (Authentic)
- **B**: RealVideo-FakeAudio (Voice cloned)
- **C**: FakeVideo-RealAudio (Face swapped)
- **D**: FakeVideo-FakeAudio (Both manipulated)

## üìÅ Project Structure

```
deepfake_detction_2/
‚îú‚îÄ‚îÄ data/                           # Dataset storage
‚îÇ   ‚îú‚îÄ‚îÄ raw/                        # Original FakeAVCeleb dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RealVideo-RealAudio/    # Category A: Authentic videos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RealVideo-FakeAudio/    # Category B: Voice cloned
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FakeVideo-RealAudio/    # Category C: Face swapped
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FakeVideo-FakeAudio/    # Category D: Both fake
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ meta_data.csv           # Dataset metadata
‚îÇ   ‚îî‚îÄ‚îÄ processed/                  # Preprocessed features
‚îÇ       ‚îú‚îÄ‚îÄ frames/                 # Extracted video frames
‚îÇ       ‚îú‚îÄ‚îÄ faces/                  # Cropped face regions
‚îÇ       ‚îú‚îÄ‚îÄ audio/                  # Extracted audio files
‚îÇ       ‚îú‚îÄ‚îÄ audio_features/         # Audio spectrograms
‚îÇ       ‚îú‚îÄ‚îÄ sync/                   # Lip-sync alignment data
‚îÇ       ‚îî‚îÄ‚îÄ fusion_features/        # Combined features
‚îú‚îÄ‚îÄ src/                            # Source code
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/              # Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ models/                     # Model architectures
‚îÇ   ‚îú‚îÄ‚îÄ training/                   # Training scripts
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/                 # Evaluation & metrics
‚îÇ   ‚îî‚îÄ‚îÄ api/                        # FastAPI deployment
‚îú‚îÄ‚îÄ models/                         # Trained model weights
‚îú‚îÄ‚îÄ results/                        # Evaluation results
‚îú‚îÄ‚îÄ notebooks/                      # Jupyter notebooks
‚îî‚îÄ‚îÄ requirements.txt                # Dependencies
```

## üîÑ Complete Workflow

### 1. Data Preprocessing

#### Frame Extraction (`src/preprocessing/extract_frames.py`)
- Extracts frames from videos at regular intervals
- Converts video files to image sequences
- **Input**: MP4 videos from `data/raw/`
- **Output**: Frame images in `data/processed/frames/`

#### Face Detection (`src/preprocessing/face_detection.py`)
- Detects and crops face regions from frames
- Uses computer vision techniques to isolate faces
- **Input**: Frame images
- **Output**: Cropped face images in `data/processed/faces/`

#### Audio Extraction (`src/preprocessing/extract_audio.py`)
- Separates audio tracks from video files
- Converts to standard format (WAV, 16kHz)
- **Input**: MP4 videos
- **Output**: Audio files in `data/processed/audio/`

#### Feature Extraction (`src/preprocessing/feature_extraction.py`)
- Converts audio to mel-spectrograms
- Creates visual embeddings from face images
- **Input**: Audio files and face images
- **Output**: Feature vectors in `data/processed/audio_features/`

#### Sync Preprocessing (`src/preprocessing/sync_preprocess.py`)
- Aligns lip movements with audio
- Creates synchronization datasets
- **Input**: Face images and audio
- **Output**: Sync data in `data/processed/sync/`

### 2. Model Training

#### Visual Model (`src/training/train_visual.py`)
- **Architecture**: ResNet-18 based CNN
- **Purpose**: Detects face manipulation artifacts
- **Input**: Face images (224x224)
- **Output**: Real/Fake classification
- **Saved**: `models/visual_model/visual_model.pth`

#### Audio Model (`src/training/train_audio.py`)
- **Architecture**: CNN for spectrogram analysis
- **Purpose**: Identifies synthetic voice patterns
- **Input**: Mel-spectrograms (64x160)
- **Output**: Real/Fake audio classification
- **Saved**: `models/audio_model/audio_model.pth`

#### Sync Model (`src/training/train_sync.py`)
- **Architecture**: LSTM-based sequence model
- **Purpose**: Detects lip-sync mismatches
- **Input**: Lip movement + audio alignment
- **Output**: Sync/Unsync classification
- **Saved**: `models/sync_model/sync_model.pth`

#### Fusion Model (`src/training/train_fusion.py`)
- **Architecture**: Multi-layer perceptron
- **Purpose**: Combines all model outputs
- **Input**: Visual + Audio + Sync predictions
- **Output**: Final 4-category classification (A/B/C/D)
- **Saved**: `models/fusion_model/fusion_model.pth`

### 3. Model Evaluation

#### Performance Metrics (`src/evaluation/evaluate.py`)
- Calculates accuracy, precision, recall, F1-score
- Generates ROC curves and confusion matrices
- **Output**: Metrics saved in `results/evaluation/`

#### Explainability (`src/evaluation/explainability.py`)
- Generates Grad-CAM visualizations
- Shows which regions influence decisions
- **Output**: Heatmaps in `results/explainability/`

### 4. API Deployment

#### FastAPI Server (`src/api/main.py`)
- **Endpoint**: `POST /predict`
- **Input**: Video file upload
- **Process**: 
  1. Extract frames and audio
  2. Run all four models
  3. Combine predictions
- **Output**: JSON with predictions and confidence scores

#### Inference Pipeline (`src/api/inference.py`)
- Handles video preprocessing
- Loads trained models
- Performs real-time prediction

## üöÄ Quick Start

### Installation
```bash
# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### Configuration
Edit `.env` file to configure data filtering:
```bash
# Filter by specific race (e.g., only Asian data)
FILTER_BY_RACE=True
SELECTED_RACES=Asian (East)

# Limit data size for faster processing
MAX_SAMPLES_PER_CATEGORY=50
MAX_TOTAL_SAMPLES=200

# Adjust detection thresholds
CONFIDENCE_THRESHOLD=0.5
DETECTION_THRESHOLD=0.7
```

### Run Complete Pipeline
```bash
# Process data, train models, and evaluate
python run_pipeline.py
```

### Manual Steps (Optional)
```bash
# Data processing only
python src/preprocessing/data_processor.py

# Train individual models
python src/training/train_visual.py
python src/training/train_audio.py
```

### Training Models
```bash
# Train individual models
python src/training/train_visual.py
python src/training/train_audio.py
python src/training/train_sync.py

# Train fusion model
python src/training/train_fusion.py
```

### Evaluation
```bash
# Evaluate all models
python src/evaluation/evaluate.py

# Generate explainability results
python src/evaluation/explainability.py
```

### Run API
```bash
# Start FastAPI server
uvicorn src.api.main:app --reload

# API will be available at http://localhost:8000
# Upload video to /predict endpoint
```

## üìä Dataset Information

**FakeAVCeleb Dataset** contains videos generated using:
- **Faceswap**: Face replacement
- **FSGAN**: Face reenactment  
- **Wav2Lip**: Lip synchronization
- **RTVC**: Voice cloning

Each video is categorized based on authenticity of visual and audio components.

## ‚öôÔ∏è Configuration Options

### Environment Variables (`.env`)

**Data Filtering:**
- `FILTER_BY_RACE`: Enable/disable race filtering
- `SELECTED_RACES`: Comma-separated list of races to include
- `FILTER_BY_GENDER`: Enable/disable gender filtering  
- `SELECTED_GENDERS`: men, women, or both
- `FILTER_BY_CATEGORY`: Enable/disable category filtering
- `SELECTED_CATEGORIES`: A, B, C, D categories

**Data Limits:**
- `MAX_SAMPLES_PER_CATEGORY`: Limit samples per category (default: 100)
- `MAX_TOTAL_SAMPLES`: Total sample limit (default: 1000)

**Model Thresholds:**
- `CONFIDENCE_THRESHOLD`: Model confidence threshold (default: 0.5)
- `DETECTION_THRESHOLD`: Detection threshold (default: 0.7)
- `SYNC_THRESHOLD`: Audio-visual sync threshold (default: 0.6)

**Processing Options:**
- `BATCH_SIZE`: Training batch size (default: 32)
- `MAX_FRAMES_PER_VIDEO`: Frames to extract per video (default: 16)
- `IMAGE_SIZE`: Image dimensions (default: 224)

## üéØ Model Performance

Results are saved in `results/evaluation/`:
- Confusion matrices for each model
- ROC curves showing detection performance
- Score distributions for real vs fake content

## üîß Key Features

- **Multi-modal Analysis**: Combines visual, audio, and sync detection
- **Real-time Inference**: FastAPI deployment for live detection
- **Explainable AI**: Grad-CAM visualizations show decision reasoning
- **Comprehensive Evaluation**: Multiple metrics and visualizations
- **Modular Design**: Each component can be trained/evaluated independently

## üìù Usage Examples

### Configuration Examples

```bash
# Process only Asian (East) male data
FILTER_BY_RACE=True
SELECTED_RACES=Asian (East)
FILTER_BY_GENDER=True
SELECTED_GENDERS=men
MAX_SAMPLES_PER_CATEGORY=25

# Process only real vs fake video (categories A and C)
FILTER_BY_CATEGORY=True
SELECTED_CATEGORIES=A,C

# High precision detection
CONFIDENCE_THRESHOLD=0.8
DETECTION_THRESHOLD=0.9
```

### API Usage

```python
# Upload video to API
import requests

with open("test_video.mp4", "rb") as f:
    response = requests.post(
        "http://localhost:8000/predict",
        files={"file": f}
    )

result = response.json()
print(f"Visual: {result['visual_pred']} ({result['visual_prob']:.3f})")
print(f"Audio: {result['audio_pred']} ({result['audio_prob']:.3f})")
print(f"Final: {result['fusion_pred']} ({result['fusion_prob']:.3f})")
```

## üéì Research Applications

This system can be used for:
- Social media content verification
- News authenticity checking
- Digital forensics
- Media literacy education
- Academic research on deepfake detection

## üìà Future Enhancements

- Real-time video stream processing
- Mobile app deployment
- Additional deepfake generation methods
- Improved model architectures
- Cloud deployment options